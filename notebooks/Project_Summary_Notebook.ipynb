{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Polygenic Risk Score Accuracy is Dependent on Local Ancestry\n",
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import necessary packages and code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# packages\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import sys\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# code\n",
    "sys.path.insert(0,\"/Users/taylorcavazos/repos/Local_Ancestry_PRS/code/\")\n",
    "from plot_correlations import *\n",
    "from plot_af_ld import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Contents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To do"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When PRS are built in Europeans they perform poorly in non-Europeans; however, our most powered GWAS are of European populations. Thus moving forward we not only need larger diverse population datsets, but also statistical approaches for generalizing these scores.\n",
    "\n",
    "![](https://media.springernature.com/full/springer-static/image/art%3A10.1038%2Fs41588-019-0379-x/MediaObjects/41588_2019_379_Fig3_HTML.png)\n",
    "\n",
    "Performance seems to decrease with decreasing proportion of European ancestry, which led me to the question of whether this accuracy may be depednent on local ancestry proportions within ancestry groups."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Simulation setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/methods.jpeg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "____\n",
    "# Simulation PRS Accuracy\n",
    "\n",
    "## Pearson's correlation of european derived PRS in African individuals by percent european ancestry at PRS variants"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are holding p-value and r2 constant at 0.01 and 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_all_params_eur_weights()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## We can improve PRS accuracy by using local ancestry specific weights "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_correlation_all_params_all_weights()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### To move forward with analysis I will limit to parameters $m=1000$ and $h2=0.5$ because although all parameter combinations reflect the same trend, this one is likely to be a closer proximity to true disease biology... hundreds of SNPs have been identified as causal for many common diseases and this isn't including the rare variants that are likely to be discovered (& are causal in my simulation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_all_weight_summary_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(1,2,figsize=(30,15))\n",
    "plot_correlation_single_eur_weights(data,ax[0],m=1000,h2=0.5)\n",
    "plot_correlation_single_all_weights(data,ax[1],m=1000,h2=0.5)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eur_weight_ceu_only = data.loc[(data[\"m\"]==1000)&(data[\"h2\"]==0.5)&(data[\"weight\"]==\"European\"),\"test_EUR_corr\"]\n",
    "\n",
    "afr_weight_ceu_low = data.loc[(data[\"m\"]==1000)&(data[\"h2\"]==0.5)&(data[\"weight\"]==\"African\"),\"ADMIX_low_eur_corr\"]\n",
    "eur_weight_ceu_low = data.loc[(data[\"m\"]==1000)&(data[\"h2\"]==0.5)&(data[\"weight\"]==\"European\"),\"ADMIX_low_eur_corr\"]\n",
    "LA_weight_ceu_low = data.loc[(data[\"m\"]==1000)&(data[\"h2\"]==0.5)&(data[\"weight\"]==\"Local ancestry \\nspecific\"),\"ADMIX_low_eur_corr\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats.ttest_ind(LA_weight_ceu_low,afr_weight_ceu_low)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Impact of decreasing African sample size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "plot_correlation_decreasing_yri_allPops()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main Takeaways\n",
    "#### (1) Local ancestry matters\n",
    "\n",
    "#### (2) Using ancestry specific weights isn't enough to achieve similar accuracy as PRS in Europeans"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Exploration of Simulation PRS -  Why doesn't the European PRS generalize across populations?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Allele frequency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_maf_bins()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linkage disequilibrium"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import msprime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tqdm\n",
    "import threading\n",
    "import math\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "\n",
    "from multiprocessing import Pool\n",
    "import time\n",
    "import pickle\n",
    "\n",
    "import random\n",
    "from collections import defaultdict\n",
    "from itertools import chain\n",
    "from operator import methodcaller"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_yri = msprime.load(\"../data/sim2/trees/tree_YRI_GWAS_nofilt.hdf\")\n",
    "tree_ceu = msprime.load(\"../data/sim2/trees/tree_CEU_GWAS_nofilt.hdf\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def return_LD_dict(tree_LD):\n",
    "    var2mut, mut2var, positions = {}, {}, {}\n",
    "    for mut in tree_LD.mutations():\n",
    "        mut2var[mut.id]=mut.site\n",
    "        var2mut[mut.site]=mut.id\n",
    "        positions[mut.site]=mut.position\n",
    "\n",
    "    tree_LD_filt = tree_LD.simplify(filter_sites=True)\n",
    "    return tree_LD_filt, var2mut, mut2var, positions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_ceu_LD, var2mut_ceu, mut2var_ceu, pos_ceu = return_LD_dict(tree_ceu)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_yri_LD, var2mut_yri, mut2var_yri, pos_yri = return_LD_dict(tree_yri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "vars_to_check = list(set(pos_ceu.keys()).intersection(pos_yri.keys()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_keys = sorted(vars_to_check)\n",
    "\n",
    "def get_dist_bin(dist):\n",
    "    if dist < 5: dist_bin = 1\n",
    "    elif dist >= 5 and dist < 10: dist_bin = 2\n",
    "    elif dist >= 10 and dist < 15: dist_bin = 3\n",
    "    elif dist >= 15 and dist < 20: dist_bin = 4\n",
    "    elif dist >= 20 and dist < 25: dist_bin = 5\n",
    "    elif dist >= 25 and dist < 30: dist_bin = 6\n",
    "    elif dist >= 30 and dist < 35: dist_bin = 7\n",
    "    elif dist >= 35 and dist < 40: dist_bin = 8\n",
    "    elif dist >= 40 and dist < 45: dist_bin = 9\n",
    "    elif dist >= 45 and dist < 50: dist_bin = 10\n",
    "    else: dist_bin = -1\n",
    "    return dist_bin\n",
    "\n",
    "def get_dist(i):\n",
    "#     df = pd.DataFrame(columns=[\"var1\",\"var2\",\"dist\"])\n",
    "    new_dict = dict.fromkeys(range(1,11), [])\n",
    "    j=i+1\n",
    "    while(j < len(list_keys) and np.absolute(pos_ceu.get(list_keys[i])-pos_ceu.get(list_keys[j])) <= 50e3):\n",
    "        dist = np.absolute(pos_ceu.get(list_keys[j])-pos_ceu.get(list_keys[i]))/1000\n",
    "        dist_bin = get_dist_bin(dist)\n",
    "        new_dict.get(dist_bin).append((list_keys[i],list_keys[j]))\n",
    "#         df = df.append({\"var1\":list_keys[i],\"var2\":list_keys[j],\"dist\":dist},ignore_index=True)\n",
    "        j+=1\n",
    "    return new_dict\n",
    "                       \n",
    "pool = Pool(processes=8)\n",
    "pairwise_dists = pool.map(get_dist,range(len(list_keys)))\n",
    "pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairwise_dists_comb = defaultdict(list)\n",
    "dict_items = map(methodcaller(\"items\"),pairwise_dists)\n",
    "for k,v in chain.from_iterable(dict_items):\n",
    "    pairwise_dists_comb[k].extend(v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dist_inds_df = pd.DataFrame(columns=[0,1,2])\n",
    "for k in pairwise_dists_comb.keys():\n",
    "    v = pairwise_dists_comb.get(k)\n",
    "    rand_inds = random.sample(v,100)\n",
    "    sub_df = pd.DataFrame(np.array(rand_inds))\n",
    "    sub_df[2] = k\n",
    "    dist_inds_df = dist_inds_df.append(sub_df,ignore_index=True)\n",
    "dist_inds_df.columns=[\"var1\",\"var2\",\"dist_bin\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ld_ceu = msprime.LdCalculator(tree_ceu)\n",
    "ld_yri = msprime.LdCalculator(tree_yri)\n",
    "\n",
    "def find_ld(ind):\n",
    "    results = {}\n",
    "    var1 = int(dist_inds_df.loc[ind,\"var1\"])\n",
    "    var2 = int(dist_inds_df.loc[ind,\"var2\"]) \n",
    "    results[ind] = (ld_ceu.get_r2(var1,var2), ld_yri.get_r2(var1,var2))\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "overall_result = []\n",
    "for key,val in dist_inds_df.groupby(\"dist_bin\").groups.items():\n",
    "    pool = Pool(processes=8)\n",
    "    ld_result = pool.map(find_ld,val)\n",
    "    overall_result.append(ld_result)\n",
    "    pool.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "overall_result = np.array(overall_result).flatten()\n",
    "overall_dicts = {}\n",
    "for d in overall_result:\n",
    "    overall_dicts = {**overall_dicts,**d}\n",
    "\n",
    "for key, val in overall_dicts.items():\n",
    "    dist_inds_df.loc[key,\"CEU_r2\"] = val[0]\n",
    "    dist_inds_df.loc[key,\"YRI_r2\"] = val[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mean_mat = dist_inds_df.groupby(\"dist_bin\").mean().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mean_mat[\"dist_bin\"] = np.arange(5,55,5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10,5))\n",
    "sns.lineplot(x=\"dist_bin\",y=\"CEU_r2\",data=mean_mat,color=\"blue\")\n",
    "sns.lineplot(x=\"dist_bin\",y=\"YRI_r2\",data=mean_mat,color=\"red\")\n",
    "plt.xticks(np.arange(5,55,5))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "results_yri = find_ld_sites(tree_yri_LD,clumped_vars,var2mut_yri,mut2var_yri,r2_threshold=0,num_threads=10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "positions_selected = results_ceu.keys()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "positions_ld = results_ceu.values().keys()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vals = results_ceu.values()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "vals[:5]"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "ld_mat = pd.DataFrame(index=causal_vars,columns=sorted(clumped_vars.astype(int)))\n",
    "for caus in tqdm.tqdm(results.keys()):\n",
    "    if results[caus] != None:\n",
    "        for non_caus in ld_mat.columns:\n",
    "            if non_caus in results[caus].keys():\n",
    "                ld_mat.loc[caus,non_caus] =  results[caus].get(non_caus)\n",
    "        "
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ld_mat.to_csv(\"../data/sim2/ld_mat_yri.txt\",sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ld_mat_ceu = pd.read_csv(\"../data/sim2/ld_mat_ceu.txt\",sep=\"\\t\",index_col=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ld_mat_yri = pd.read_csv(\"../data/sim2/ld_mat_yri.txt\",sep=\"\\t\",index_col=0)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ld_mat_yri.dropna(how=\"all\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "yri_sumstats = pd.read_csv(\"../data/sim2/emp_prs/yri_comm_maf_0.01_sum_stats_m_1000_h2_0.5.txt\",index_col=0,sep=\"\\t\")\n",
    "ceu_sumstats = pd.read_csv(\"../data/sim2/emp_prs/comm_maf_0.01_sum_stats_m_1000_h2_0.5.txt\",index_col=0,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "yri_sumstats[(yri_sumstats.index > 900000)& (yri_sumstats.index < 1000000)].sort_values(by=\"p-value\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "ceu_sumstats[(ceu_sumstats.index > 900000)& (ceu_sumstats.index < 1000000)].sort_values(by=\"p-value\")"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "var2mut_ceu.get(981803)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "var2mut_ceu.get(983500)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ld_calc = msprime.LdCalculator(tree_ceu)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ld_calc.get_r2(981803,983500)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ld_calc.get_r2(666318,667473)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "var2mut_yri.get(981803)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "var2mut_yri.get(985644)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ld_calc = msprime.LdCalculator(tree_yri)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "ld_calc.get_r2(981803,985644)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "ld_mat_ceu.loc[[981803],:].dropna(axis=1).T.head(60).tail(10)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "ld_mat_yri.loc[[981803],:].dropna(axis=1).T"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "def return_max_r2(results,var):\n",
    "    if results.get(var) != None:\n",
    "        return max(results.get(var).items(), key=operator.itemgetter(1))\n",
    "    else: \n",
    "        return None, None\n",
    "\n",
    "best_tag_ceu = 0\n",
    "best_tag_yri = 0\n",
    "not_tagged_ceu = 0\n",
    "not_tagged_yri = 0\n",
    "not_tagged_either = 0\n",
    "\n",
    "for var in tqdm.tqdm(causal_vars):\n",
    "    ceu_var,ceu_r2 = return_max_r2(results_ceu,var)\n",
    "    yri_var,yri_r2 = return_max_r2(results_yri,var)\n",
    "    \n",
    "    if ceu_var == None:\n",
    "        not_tagged_ceu+=1\n",
    "    if yri_var == None:\n",
    "        not_tagged_yri+=1\n",
    "        \n",
    "    if ceu_var == None and yri_var != None and yri_r2>0.01:\n",
    "        best_tag_yri+=1\n",
    "#         best_tag_yri.append(var)\n",
    "    elif ceu_var != None and yri_var == None and ceu_r2>0.01:\n",
    "        best_tag_ceu+=1\n",
    "#         best_tag_ceu.append(var)\n",
    "    elif ceu_var != None and yri_var != None:\n",
    "#         print(var,ceu_r2,yri_r2)\n",
    "        if ceu_r2 > yri_r2 and ceu_r2>0.01:\n",
    "            best_tag_ceu+=1\n",
    "#             best_tag_ceu.append(var)\n",
    "        elif yri_r2 >= ceu_r2 and yri_r2>0.01: \n",
    "            best_tag_yri+=1\n",
    "#             best_tag_yri.append(var)\n",
    "    else: \n",
    "        not_tagged_either+=1"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "best_tag_ceu"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "best_tag_yri"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "not_tagged_either"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "558+334+108"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "not_tagged_ceu"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "not_tagged_yri"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "not_tagged_either"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "613+382"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(causal_vars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Deep dive into causal variants\n",
    "\n",
    "(1) If they are present in the summary statistics do they have the effect size? What about the p-values?\n",
    "* Make plot of one simulation (heatmap of some sort)\n",
    "* Come up with plot for summarizing all simulations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tree_yri = msprime.load(\"../data/sim2/trees/tree_YRI_GWAS_nofilt.hdf\")\n",
    "tree_ceu = msprime.load(\"../data/sim2/trees/tree_CEU_GWAS_nofilt.hdf\")\n",
    "\n",
    "causal_vars = np.linspace(0, tree_ceu.num_sites, m, dtype=int,endpoint=False)\n",
    "\n",
    "yri_sumstats = pd.read_csv(\"../data/sim2/emp_prs/yri_comm_maf_0.01_sum_stats_m_1000_h2_0.5.txt\",index_col=0,sep=\"\\t\")\n",
    "ceu_sumstats = pd.read_csv(\"../data/sim2/emp_prs/comm_maf_0.01_sum_stats_m_1000_h2_0.5.txt\",index_col=0,sep=\"\\t\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yri_sumstats.loc[yri_sumstats.OR==0,\"OR\"] = 1\n",
    "ceu_sumstats.loc[ceu_sumstats.OR==0,\"OR\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_pres_yri = yri_sumstats.reindex(causal_vars).dropna()\n",
    "causal_pres_yri.columns = [\"OR_YRI\",\"PVAL_YRI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "causal_pres_ceu =ceu_sumstats.reindex(causal_vars).dropna()\n",
    "causal_pres_ceu.columns = [\"OR_CEU\",\"PVAL_CEU\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data = pd.concat([causal_pres_ceu,causal_pres_yri],sort=False,axis=1).dropna()\n",
    "data[\"CEU\"] = -1*np.log10(data[\"PVAL_CEU\"])\n",
    "data[\"YRI\"] = -1*np.log10(data[\"PVAL_YRI\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_long = data.reset_index().melt(id_vars=\"var_id\",value_vars=[\"CEU\",\"YRI\"],)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(x, y, size, color):\n",
    "    fig, ax = plt.subplots(figsize=(36,5))\n",
    "    \n",
    "    # Mapping from column names to integer coordinates\n",
    "    x_labels = [v for v in sorted(x.unique())]\n",
    "    y_labels = [v for v in sorted(y.unique())]\n",
    "    x_to_num = {p[1]:p[0] for p in enumerate(x_labels)} \n",
    "    y_to_num = {p[1]:p[0] for p in enumerate(y_labels)} \n",
    "    \n",
    "    size_scale = 5\n",
    "    sns.scatterplot(\n",
    "        x=x.map(x_to_num), # Use mapping for x\n",
    "        y=y.map(y_to_num), # Use mapping for y\n",
    "        s=size * size_scale, # Vector of square sizes, proportional to size parameter\n",
    "        marker='o', # Use square as scatterplot marker\n",
    "        hue = color,\n",
    "        palette = \"bwr\",\n",
    "        ax=ax,\n",
    "        legend=False\n",
    "    )\n",
    "    \n",
    "    # Show column labels on the axes\n",
    "    ax.set_xticks([x_to_num[v] for v in x_labels])\n",
    "    ax.set_xticklabels(x_labels, rotation=45, horizontalalignment='right',fontsize=18)\n",
    "    ax.set_yticks([y_to_num[v] for v in y_labels])\n",
    "    ax.set_yticklabels(y_labels)\n",
    "    \n",
    "    \n",
    "    ax.grid(False, 'major')\n",
    "    ax.grid(True, 'minor')\n",
    "    ax.set_xticks([t + 0.5 for t in ax.get_xticks()], minor=True)\n",
    "    ax.set_yticks([t + 0.5 for t in ax.get_yticks()], minor=True)\n",
    "    \n",
    "    \n",
    "    ax.set_xlim([-0.5, max([v for v in x_to_num.values()]) + 0.5]) \n",
    "    ax.set_ylim([-0.5, max([v for v in y_to_num.values()]) + 0.5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ind in data_long.index:\n",
    "    var = data_long.loc[ind,\"var_id\"]\n",
    "    if data_long.loc[ind,\"variable\"]==\"CEU\":\n",
    "        data_long.loc[ind,\"color\"] = data.loc[var,\"OR_CEU\"]\n",
    "    else: data_long.loc[ind,\"color\"] = data.loc[var,\"OR_YRI\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "heatmap(data_long[\"var_id\"],data_long[\"variable\"],data_long[\"value\"],np.log(data_long[\"color\"]))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___\n",
    "# Next Steps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test different SNP selection approaches\n",
    "* African, European (done), or Meta selected SNPs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
